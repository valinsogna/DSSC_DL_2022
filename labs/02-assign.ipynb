{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework\n",
    "\n",
    "1. Complete the Python implementation of the backpropagation exercise in the **Backpropagation** section here above (cell `# try it in Python as homework!`)\n",
    "    - Create the calculations for obtaining $y$ in PyTorch **using only PyTorch methods and routines**\n",
    "    - Calculate the gradient\n",
    "    - Check the values of the gradients and see if it is correct w.r.t. the manual calculations\n",
    "2. Given the multilayer perceptron defined during the exercises from lab 1:\n",
    "    - Create 10 random datapoints (with any function you wish, it can be `rand`, `randn`...) and feed them into the network\n",
    "    - Given the output, calculate the Cross-Entropy loss with respect to the ground truth $[1,2,3,4,1,2,3,4,1,2]$ (classes from 1 to 4). Cross-Entropy loss:\n",
    "        \n",
    "        $$ CE(\\mathbf{y}, \\hat{\\mathbf{y}}) = - \\sum_{i=1}^{10} \\hat{y}_i \\log(y_i)$$\n",
    "        \n",
    "        where $y_i$ is the one-hot encoding of the $i$-th datapoint. For instance, $y_1 = [1,0,0,0]$.\n",
    "        **_Note: there is an extremely handy PyTorch function for getting a one-hot encoding out of a vector, so don't try anything fancy._**\n",
    "    - Backpropagate the error along the network and inspect the gradient of the parameters connecting the input layer and the first hidden layer.\n",
    "3. Execute the python script `utils/randomized_backpropagation_formula.py`. This creates a formula $f(\\mathbf{x})$ with randomized operators and values. Create the computational graph from this formula, do (by hand) the forward pass, then calculate (by hand) $\\nabla f(\\mathbf{x})$ using the backward gradient computation. Do the same calculation on PyTorch to check the correctness of your calculations. _Note: The formula created by this script is linked to your name and surname, which you have to input before_. The solution to this exercise _should_ be submitted as a scan/good quality picture of a piece of paper (or you can do it on a touch screen and submit the image...), but other formats are acceptable as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "**Backpropagation**\n",
    "\n",
    "Let us suppose we have the following calculation\n",
    "\n",
    "$\\mathbf{x} = [1,~2,~-1,~3,~5]$\n",
    "\n",
    "$ y = f(\\mathbf{x}) = \\log\\{[\\exp (x_1 * x_2 )]^2 + \\sin (x_3 + x_4 + x_5) \\cdot x_5\\}$\n",
    "\n",
    "Find\n",
    "\n",
    "$\\nabla f(\\mathbf{x})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], requires_grad=True)\n",
      "tensor([2.], requires_grad=True)\n",
      "tensor([-1.], requires_grad=True)\n",
      "tensor([3.], requires_grad=True)\n",
      "tensor([5.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.tensor([1.0], requires_grad=True)\n",
    "x2 = torch.tensor([2.0], requires_grad=True)\n",
    "x3 = torch.tensor([-1.0], requires_grad=True)\n",
    "x4 = torch.tensor([3.0], requires_grad=True)\n",
    "x5 = torch.tensor([5.0], requires_grad=True)\n",
    "\n",
    "print(x1)\n",
    "print(x2)\n",
    "print(x3)\n",
    "print(x4)\n",
    "print(x5)\n",
    "\n",
    "# x = torch.tensor([1,2,-1,3,5], dtype=torch.float32, requires_grad=True)\n",
    "# print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2., grad_fn=<DotBackward0>)\n",
      "tensor([7.], grad_fn=<AddBackward0>)\n",
      "tensor(7.3891, grad_fn=<ExpBackward0>)\n",
      "tensor(54.5982, grad_fn=<PowBackward0>)\n",
      "tensor([0.6570], grad_fn=<SinBackward0>)\n",
      "tensor(3.2849, grad_fn=<DotBackward0>)\n",
      "tensor(57.8831, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "The value of function f(x) evalkuated in x is:\n",
      "tensor(4.0584, grad_fn=<LogBackward0>)\n"
     ]
    }
   ],
   "source": [
    "a = torch.matmul(x1, x2)\n",
    "print(a)\n",
    "b = x3 + x4 + x5\n",
    "print(b)\n",
    "c = a.exp()\n",
    "print(c)\n",
    "d = torch.pow(c, 2)\n",
    "print(d)\n",
    "g = b.sin()\n",
    "print(g)\n",
    "h = torch.matmul(g, x5)\n",
    "print(h)\n",
    "i = d + h\n",
    "print(i)\n",
    "y = torch.log(i)\n",
    "print(\"\\n\")\n",
    "print(\"The value of function f(x) evaluated in x is:\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.7730])\n",
      "tensor([1.8865])\n",
      "tensor([0.0651])\n",
      "tensor([0.0651])\n",
      "tensor([0.0765])\n"
     ]
    }
   ],
   "source": [
    "y.backward()\n",
    "print(x1.grad)\n",
    "print(x2.grad)\n",
    "print(x3.grad)\n",
    "print(x4.grad)\n",
    "print(x5.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From manual calculations performed in class, we expected:\n",
    "\n",
    "- $\\partial f/\\partial x_1 = 4.14$\n",
    "- $\\partial f/\\partial x_2 = 2.07$\n",
    "- $\\partial f/\\partial x_3 = 0.08$\n",
    "- $\\partial f/\\partial x_4 = 0.08$\n",
    "- $\\partial f/\\partial x_4 = 0.093$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from previous cell in Part 2, the manual calculations are slightly different since we made use of approximations in class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was the artificial network built in the first assignement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, my_bias=False):\n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(5, 11, bias=my_bias),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(11, 16, bias=my_bias),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16, 13, bias=my_bias),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(13, 8, bias=my_bias),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(8, 4, bias=my_bias),\n",
    "            torch.nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.layers(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is an instance of it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=5, out_features=11, bias=False)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=11, out_features=16, bias=False)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=16, out_features=13, bias=False)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=13, out_features=8, bias=False)\n",
       "    (7): ReLU()\n",
       "    (8): Linear(in_features=8, out_features=4, bias=False)\n",
       "    (9): Softmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLP()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feed 10 datapoints to the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.7291, -0.5698,  1.9403, -1.6810,  0.2411],\n",
      "        [ 0.0705, -0.6490,  1.1611,  0.8965, -1.1248],\n",
      "        [-0.7839,  0.7453, -0.1454,  0.0465,  0.1366],\n",
      "        [ 0.1248,  0.0842, -0.2018, -0.3602,  1.1874],\n",
      "        [ 0.4999, -0.1815, -0.1066, -1.7452, -0.0511],\n",
      "        [ 0.8780,  0.8388,  0.6155,  2.5522, -1.6753],\n",
      "        [-2.5104,  0.3058, -1.6485,  0.5160, -0.2190],\n",
      "        [ 0.1298, -0.8935,  0.2552,  0.1859,  0.0697],\n",
      "        [ 0.5437, -1.6091,  0.2296, -0.6091,  0.0412],\n",
      "        [ 0.5275, -1.6586, -1.5570, -0.2524, -1.4706]])\n",
      "\n",
      "\n",
      "tensor([[0.2505, 0.2488, 0.2509, 0.2497],\n",
      "        [0.2494, 0.2488, 0.2523, 0.2495],\n",
      "        [0.2504, 0.2499, 0.2501, 0.2497],\n",
      "        [0.2508, 0.2491, 0.2509, 0.2492],\n",
      "        [0.2478, 0.2490, 0.2527, 0.2505],\n",
      "        [0.2488, 0.2494, 0.2522, 0.2496],\n",
      "        [0.2481, 0.2502, 0.2519, 0.2498],\n",
      "        [0.2485, 0.2505, 0.2509, 0.2502],\n",
      "        [0.2475, 0.2494, 0.2531, 0.2500],\n",
      "        [0.2455, 0.2510, 0.2537, 0.2498]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "X = torch.randn(10, 5) #Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1\n",
    "print(X)\n",
    "print(\"\\n\")\n",
    "conf_y_hat = model(X)\n",
    "print(conf_y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predicted classes for the 10 points are then:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 3, 1, 3, 3, 3, 3, 3, 3, 3])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = torch.argmax(conf_y_hat, dim=1) + 1 # in order to be confrontable with the ground_truth we add 1 \n",
    "y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the ground truth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 1, 2, 3, 4, 1, 2])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We keep it with float point since the torch.nn.functional.one_hot function doesn't allow integers\n",
    "ground_truth = torch.tensor([1,2,3,4,1,2,3,4,1,2]) # dtype=torch.int) or ground_truth.int() in case\n",
    "ground_truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Cross-Entropy Loss:\n",
    "$$ CE(\\mathbf{y}, \\hat{\\mathbf{y}}) = - \\sum_{i=1}^{10} \\hat{y}_i \\log(y_i)$$\n",
    "        \n",
    "where $y_i$ is the one-hot encoding of the $i$-th datapoint. For instance, $y_1 = [1,0,0,0]$.\n",
    "\n",
    "**_Note: there is an extremely handy PyTorch function for getting a one-hot encoding out of a vector, so don't try anything fancy._**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ce_loss(y_hat, y):\n",
    "    # First we need to transform y in one-hot encoding tensor of shape 4 x 10.\n",
    "    # To do it we must rescale the ground_truth values in compliance with the function torch.nn.functional.one_hot,\n",
    "    # since in order to work num_classes must be greater than the biggest values among ground_truth.\n",
    "    scaled_truth = y-1\n",
    "    # Then we apply the function one_hot (if num_classes=-1 then num_classes=max_value in ground_truth + 1)\n",
    "    y_hot = torch.nn.functional.one_hot(scaled_truth, num_classes=-1) \n",
    "    loss = (-1) * ((y_hat-1) @ y_hot.log()).sum()\n",
    "    loss.int()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's evaluate it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expected scalar type Long but found Float",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/valeria/Desktop/Corsi/DL/DSSC_DL_2022/labs/02-assign.ipynb Cell 28'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/valeria/Desktop/Corsi/DL/DSSC_DL_2022/labs/02-assign.ipynb#ch0000041?line=0'>1</a>\u001b[0m loss \u001b[39m=\u001b[39m ce_loss(y_hat, ground_truth)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/valeria/Desktop/Corsi/DL/DSSC_DL_2022/labs/02-assign.ipynb#ch0000041?line=1'>2</a>\u001b[0m loss\n",
      "\u001b[1;32m/Users/valeria/Desktop/Corsi/DL/DSSC_DL_2022/labs/02-assign.ipynb Cell 26'\u001b[0m in \u001b[0;36mce_loss\u001b[0;34m(y_hat, y)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/valeria/Desktop/Corsi/DL/DSSC_DL_2022/labs/02-assign.ipynb#ch0000038?line=5'>6</a>\u001b[0m \u001b[39m# Then we apply the function one_hot (if num_classes=-1 then num_classes=max_value in ground_truth + 1)\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/valeria/Desktop/Corsi/DL/DSSC_DL_2022/labs/02-assign.ipynb#ch0000038?line=6'>7</a>\u001b[0m y_hot \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mnn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mone_hot(scaled_truth, num_classes\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/valeria/Desktop/Corsi/DL/DSSC_DL_2022/labs/02-assign.ipynb#ch0000038?line=7'>8</a>\u001b[0m loss \u001b[39m=\u001b[39m (\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m ((y_hat\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m) \u001b[39m@\u001b[39;49m y_hot\u001b[39m.\u001b[39;49mlog())\u001b[39m.\u001b[39msum()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/valeria/Desktop/Corsi/DL/DSSC_DL_2022/labs/02-assign.ipynb#ch0000038?line=8'>9</a>\u001b[0m loss\u001b[39m.\u001b[39mint()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/valeria/Desktop/Corsi/DL/DSSC_DL_2022/labs/02-assign.ipynb#ch0000038?line=9'>10</a>\u001b[0m \u001b[39mreturn\u001b[39;00m loss\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expected scalar type Long but found Float"
     ]
    }
   ],
   "source": [
    "loss = ce_loss(y_hat, ground_truth)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "one_hot is only applicable to index tensor.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/valeria/Desktop/Corsi/DL/DSSC_DL_2022/labs/02-assign.ipynb Cell 28'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/valeria/Desktop/Corsi/DL/DSSC_DL_2022/labs/02-assign.ipynb#ch0000040?line=0'>1</a>\u001b[0m loss \u001b[39m=\u001b[39m ce_loss(y_hat, ground_truth)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/valeria/Desktop/Corsi/DL/DSSC_DL_2022/labs/02-assign.ipynb#ch0000040?line=1'>2</a>\u001b[0m loss\n",
      "\u001b[1;32m/Users/valeria/Desktop/Corsi/DL/DSSC_DL_2022/labs/02-assign.ipynb Cell 26'\u001b[0m in \u001b[0;36mce_loss\u001b[0;34m(y_hat, y)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/valeria/Desktop/Corsi/DL/DSSC_DL_2022/labs/02-assign.ipynb#ch0000038?line=0'>1</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mce_loss\u001b[39m(y_hat, y):\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/valeria/Desktop/Corsi/DL/DSSC_DL_2022/labs/02-assign.ipynb#ch0000038?line=1'>2</a>\u001b[0m     \u001b[39m# first transform y in one-hot encoding tensor:\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/valeria/Desktop/Corsi/DL/DSSC_DL_2022/labs/02-assign.ipynb#ch0000038?line=2'>3</a>\u001b[0m     y_hot \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mnn\u001b[39m.\u001b[39;49mfunctional\u001b[39m.\u001b[39;49mone_hot(y, num_classes\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m) \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/valeria/Desktop/Corsi/DL/DSSC_DL_2022/labs/02-assign.ipynb#ch0000038?line=4'>5</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m (\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m (y_hat \u001b[39m*\u001b[39m y_hot\u001b[39m.\u001b[39mlog())\u001b[39m.\u001b[39msum()\n",
      "\u001b[0;31mRuntimeError\u001b[0m: one_hot is only applicable to index tensor."
     ]
    }
   ],
   "source": [
    "loss = ce_loss(y_hat, ground_truth)\n",
    "loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input your name, then press ENTER: Valeria Insogna\n",
    "\n",
    "f(X) =  exp((sin(x1 + x2) / ReLU(x3 + x4)) - x5)\n",
    "\n",
    "Your values\n",
    "{'x1': 3, 'x2': 4, 'x3': -2, 'x4': -1, 'x5': -2}\n",
    "\n",
    "Calculate ∇f(X) [NB: if division by 0, change the value(s) of X responsible for that]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.], requires_grad=True)\n",
      "tensor([4.], requires_grad=True)\n",
      "tensor([2.], requires_grad=True)\n",
      "tensor([-1.], requires_grad=True)\n",
      "tensor([-2.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x_1 = torch.tensor([3.0], requires_grad=True)\n",
    "x_2 = torch.tensor([4.0], requires_grad=True)\n",
    "x_3 = torch.tensor([2.0], requires_grad=True) #I have change it from -2 to +2 otherwise there is a  division with denominator 0! \n",
    "x_4 = torch.tensor([-1.0], requires_grad=True)\n",
    "x_5 = torch.tensor([-2.0], requires_grad=True)\n",
    "\n",
    "print(x_1)\n",
    "print(x_2)\n",
    "print(x_3)\n",
    "print(x_4)\n",
    "print(x_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first evaluate the forward function step by step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7.], grad_fn=<AddBackward0>)\n",
      "tensor([1.], grad_fn=<AddBackward0>)\n",
      "tensor([0.6570], grad_fn=<SinBackward0>)\n",
      "tensor([1.], grad_fn=<ReluBackward0>)\n",
      "tensor([0.6570], grad_fn=<DivBackward0>)\n",
      "tensor([2.6570], grad_fn=<SubBackward0>)\n",
      "tensor([14.2533], grad_fn=<ExpBackward0>)\n"
     ]
    }
   ],
   "source": [
    "a = x_1 + x_2\n",
    "print(a)\n",
    "b = x_3 + x_4\n",
    "print(b)\n",
    "c = a.sin()\n",
    "print(c)\n",
    "d = torch.nn.functional.relu(b)\n",
    "print(d)\n",
    "e = c/d\n",
    "print(e)\n",
    "f = e - x_5\n",
    "print(f)\n",
    "g = f.exp()\n",
    "print(g)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the image of the manual calculations for the backward propagation:\n",
    "\n",
    "![](ex2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's verify the manual counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10.7456])\n",
      "tensor([10.7456])\n",
      "tensor([-9.3642])\n",
      "tensor([-9.3642])\n",
      "tensor([-14.2533])\n"
     ]
    }
   ],
   "source": [
    "g.backward()\n",
    "print(x_1.grad)\n",
    "print(x_2.grad)\n",
    "print(x_3.grad)\n",
    "print(x_4.grad)\n",
    "print(x_5.grad)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0f658439eb39566f305aee04d580d1e9b360828ab059e2dbe01d2bb3072c08a2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('DL_Lab')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
