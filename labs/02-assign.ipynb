{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework\n",
    "\n",
    "1. Complete the Python implementation of the backpropagation exercise in the **Backpropagation** section here above (cell `# try it in Python as homework!`)\n",
    "    - Create the calculations for obtaining $y$ in PyTorch **using only PyTorch methods and routines**\n",
    "    - Calculate the gradient\n",
    "    - Check the values of the gradients and see if it is correct w.r.t. the manual calculations\n",
    "2. Given the multilayer perceptron defined during the exercises from lab 1:\n",
    "    - Create 10 random datapoints (with any function you wish, it can be `rand`, `randn`...) and feed them into the network\n",
    "    - Given the output, calculate the Cross-Entropy loss with respect to the ground truth $[1,2,3,4,1,2,3,4,1,2]$ (classes from 1 to 4). Cross-Entropy loss:\n",
    "        \n",
    "        $$ CE(\\mathbf{y}, \\hat{\\mathbf{y}}) = - \\sum_{i=1}^{10} \\hat{y}_i \\log(y_i)$$\n",
    "        \n",
    "        where $y_i$ is the one-hot encoding of the $i$-th datapoint. For instance, $y_1 = [1,0,0,0]$.\n",
    "        **_Note: there is an extremely handy PyTorch function for getting a one-hot encoding out of a vector, so don't try anything fancy._**\n",
    "    - Backpropagate the error along the network and inspect the gradient of the parameters connecting the input layer and the first hidden layer.\n",
    "3. Execute the python script `utils/randomized_backpropagation_formula.py`. This creates a formula $f(\\mathbf{x})$ with randomized operators and values. Create the computational graph from this formula, do (by hand) the forward pass, then calculate (by hand) $\\nabla f(\\mathbf{x})$ using the backward gradient computation. Do the same calculation on PyTorch to check the correctness of your calculations. _Note: The formula created by this script is linked to your name and surname, which you have to input before_. The solution to this exercise _should_ be submitted as a scan/good quality picture of a piece of paper (or you can do it on a touch screen and submit the image...), but other formats are acceptable as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "**Backpropagation**\n",
    "\n",
    "Let us suppose we have the following calculation\n",
    "\n",
    "$\\mathbf{x} = [1,~2,~-1,~3,~5]$\n",
    "\n",
    "$ y = f(\\mathbf{x}) = \\log\\{[\\exp (x_1 * x_2 )]^2 + \\sin (x_3 + x_4 + x_5) \\cdot x_5\\}$\n",
    "\n",
    "Find\n",
    "\n",
    "$\\nabla f(\\mathbf{x})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], requires_grad=True)\n",
      "tensor([2.], requires_grad=True)\n",
      "tensor([-1.], requires_grad=True)\n",
      "tensor([3.], requires_grad=True)\n",
      "tensor([5.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.tensor([1.0], requires_grad=True)\n",
    "x2 = torch.tensor([2.0], requires_grad=True)\n",
    "x3 = torch.tensor([-1.0], requires_grad=True)\n",
    "x4 = torch.tensor([3.0], requires_grad=True)\n",
    "x5 = torch.tensor([5.0], requires_grad=True)\n",
    "\n",
    "print(x1)\n",
    "print(x2)\n",
    "print(x3)\n",
    "print(x4)\n",
    "print(x5)\n",
    "\n",
    "# x = torch.tensor([1,2,-1,3,5], dtype=torch.float32, requires_grad=True)\n",
    "# print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2., grad_fn=<DotBackward0>)\n",
      "tensor([7.], grad_fn=<AddBackward0>)\n",
      "tensor(7.3891, grad_fn=<ExpBackward0>)\n",
      "tensor(54.5982, grad_fn=<PowBackward0>)\n",
      "tensor([0.6570], grad_fn=<SinBackward0>)\n",
      "tensor(3.2849, grad_fn=<DotBackward0>)\n",
      "tensor(57.8831, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "The value of function f(x) evaluated in x is:\n",
      "tensor(4.0584, grad_fn=<LogBackward0>)\n"
     ]
    }
   ],
   "source": [
    "a = torch.matmul(x1, x2)\n",
    "print(a)\n",
    "b = x3 + x4 + x5\n",
    "print(b)\n",
    "c = a.exp()\n",
    "print(c)\n",
    "d = torch.pow(c, 2)\n",
    "print(d)\n",
    "g = b.sin()\n",
    "print(g)\n",
    "h = torch.matmul(g, x5)\n",
    "print(h)\n",
    "i = d + h\n",
    "print(i)\n",
    "y = torch.log(i)\n",
    "print(\"\\n\")\n",
    "print(\"The value of function f(x) evaluated in x is:\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.7730])\n",
      "tensor([1.8865])\n",
      "tensor([0.0651])\n",
      "tensor([0.0651])\n",
      "tensor([0.0765])\n"
     ]
    }
   ],
   "source": [
    "y.backward()\n",
    "print(x1.grad)\n",
    "print(x2.grad)\n",
    "print(x3.grad)\n",
    "print(x4.grad)\n",
    "print(x5.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From manual calculations performed in class, we expected:\n",
    "\n",
    "- $\\partial f/\\partial x_1 = 4.14$\n",
    "- $\\partial f/\\partial x_2 = 2.07$\n",
    "- $\\partial f/\\partial x_3 = 0.08$\n",
    "- $\\partial f/\\partial x_4 = 0.08$\n",
    "- $\\partial f/\\partial x_4 = 0.093$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from previous cell in Part 2, the manual calculations are slightly different since we made use of approximations in class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was the artificial network built in the first assignement:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, my_bias=False):\n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(5, 11, bias=my_bias),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(11, 16, bias=my_bias),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16, 13, bias=my_bias),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(13, 8, bias=my_bias),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(8, 4, bias=my_bias),\n",
    "            torch.nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.layers(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is an instance of it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=5, out_features=11, bias=False)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=11, out_features=16, bias=False)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=16, out_features=13, bias=False)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=13, out_features=8, bias=False)\n",
       "    (7): ReLU()\n",
       "    (8): Linear(in_features=8, out_features=4, bias=False)\n",
       "    (9): Softmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLP()\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feed 10 datapoints to the network:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.6556e+00,  1.0257e+00,  3.5750e-01, -1.8889e-01, -1.0208e+00],\n",
      "        [ 5.8702e-02, -7.0612e-01, -1.9208e-01,  3.1658e-02,  1.0980e-01],\n",
      "        [ 2.1394e-01,  8.8203e-01,  1.6487e+00,  1.2860e+00, -6.9151e-01],\n",
      "        [-1.3292e-03,  5.7954e-01, -2.7523e-01, -1.0175e+00, -4.9845e-01],\n",
      "        [ 2.7703e-01,  1.4546e-01,  6.9313e-01,  5.8724e-01,  1.1743e-01],\n",
      "        [-1.8104e+00, -5.4586e-01, -7.4822e-01, -1.2569e+00, -3.9539e-01],\n",
      "        [ 1.6971e+00, -5.1476e-01,  1.6629e+00,  3.3935e-01,  1.6295e+00],\n",
      "        [-3.8369e-01,  1.2811e+00, -1.1413e-01, -7.1252e-01,  6.9003e-01],\n",
      "        [ 4.0194e-01, -1.9318e+00,  1.2945e+00,  5.1953e-02, -2.2239e+00],\n",
      "        [-1.7997e+00,  6.1693e-01, -3.5466e-01,  9.1145e-01, -8.8045e-01]],\n",
      "       requires_grad=True)\n",
      "\n",
      "\n",
      "tensor([[0.2466, 0.2530, 0.2487, 0.2516],\n",
      "        [0.2491, 0.2506, 0.2500, 0.2502],\n",
      "        [0.2452, 0.2540, 0.2485, 0.2522],\n",
      "        [0.2484, 0.2499, 0.2512, 0.2506],\n",
      "        [0.2474, 0.2524, 0.2492, 0.2510],\n",
      "        [0.2486, 0.2515, 0.2496, 0.2504],\n",
      "        [0.2433, 0.2566, 0.2469, 0.2532],\n",
      "        [0.2491, 0.2497, 0.2511, 0.2501],\n",
      "        [0.2437, 0.2559, 0.2483, 0.2521],\n",
      "        [0.2446, 0.2551, 0.2483, 0.2520]], grad_fn=<SoftmaxBackward0>)\n"
     ]
    }
   ],
   "source": [
    "X = torch.randn(10, 5,requires_grad=True) #Returns a tensor filled with random numbers from a normal distribution with mean 0 and variance 1\n",
    "print(X)\n",
    "print(\"\\n\")\n",
    "conf_y_hat = model(X)\n",
    "print(conf_y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predicted classes for the 10 points are then:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 2, 3, 2, 2, 2, 3, 2, 2])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = torch.argmax(conf_y_hat, dim=1) + 1# in order to be confrontable with the ground_truth we add 1 \n",
    "y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the ground truth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3, 4, 1, 2, 3, 4, 1, 2])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We keep it with float point since the torch.nn.functional.one_hot function doesn't allow integers\n",
    "ground_truth = torch.tensor([1,2,3,4,1,2,3,4,1,2]) # dtype=torch.int) or ground_truth.int() in case\n",
    "ground_truth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Cross-Entropy Loss:\n",
    "$$ CE(\\mathbf{y}, \\hat{\\mathbf{y}}) = - \\sum_{i=1}^{10} \\hat{y}_i \\log(y_i)$$\n",
    "        \n",
    "where $y_i$ is the one-hot encoding of the $i$-th datapoint. For instance, $y_1 = [1,0,0,0]$.\n",
    "\n",
    "**_Note: there is an extremely handy PyTorch function for getting a one-hot encoding out of a vector, so don't try anything fancy._**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ce_loss(y_hat, y):\n",
    "    # First we need to transform y in one-hot encoding tensor of shape 4 x 10.\n",
    "    # To do it we must rescale the ground_truth values in compliance with the function torch.nn.functional.one_hot,\n",
    "    # since in order to work num_classes must be greater than the biggest values among ground_truth.\n",
    "    scaled_truth = y-1\n",
    "    # Then we apply the function one_hot (if num_classes=-1 then num_classes=max_value in ground_truth + 1)\n",
    "    # and taking the transpose for the matrix multiplication:\n",
    "    y_hot = torch.nn.functional.one_hot(scaled_truth, num_classes=-1)\n",
    "    loss = (-1) * ((y_hot.float() @ y_hat.T.log())).sum()\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's evaluate it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "138.65863037109375"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss = ce_loss(conf_y_hat, ground_truth)\n",
    "loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.2386e-03, -6.5058e-04,  1.6550e-03,  1.0973e-03, -8.4768e-04],\n",
      "        [-8.5306e-04, -1.8414e-03,  4.0025e-03,  3.2193e-03,  4.5485e-03],\n",
      "        [-6.3291e-05, -2.3298e-04,  8.9059e-04,  1.8697e-03, -1.7991e-03],\n",
      "        [-2.4466e-03, -9.8706e-05,  2.3867e-03, -7.8037e-03,  1.0262e-05],\n",
      "        [-4.3372e-03, -3.3403e-03,  2.3361e-03,  2.7897e-03,  5.9572e-04],\n",
      "        [-1.1788e-03, -6.7659e-04,  1.0574e-03,  1.8217e-03, -1.3845e-03],\n",
      "        [ 2.7041e-05, -9.5241e-04,  1.8670e-03,  3.9108e-05,  7.1852e-04],\n",
      "        [ 1.6376e-03,  1.6944e-03,  2.3387e-03, -6.0555e-03, -1.0176e-03],\n",
      "        [-2.9756e-03, -9.7615e-04,  2.5698e-04,  1.7374e-03, -1.8373e-03],\n",
      "        [-1.8865e-03, -1.6928e-03,  1.4776e-03,  1.2648e-03, -9.2947e-04]])\n"
     ]
    }
   ],
   "source": [
    "loss.backward()\n",
    "print(X.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input your name, then press ENTER: Valeria Insogna\n",
    "\n",
    "f(X) =  exp((sin(x1 + x2) / ReLU(x3 + x4)) - x5)\n",
    "\n",
    "Your values\n",
    "{'x1': 3, 'x2': 4, 'x3': -2, 'x4': -1, 'x5': -2}\n",
    "\n",
    "Calculate ∇f(X) [NB: if division by 0, change the value(s) of X responsible for that]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.], requires_grad=True)\n",
      "tensor([4.], requires_grad=True)\n",
      "tensor([2.], requires_grad=True)\n",
      "tensor([-1.], requires_grad=True)\n",
      "tensor([-2.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x_1 = torch.tensor([3.0], requires_grad=True)\n",
    "x_2 = torch.tensor([4.0], requires_grad=True)\n",
    "x_3 = torch.tensor([2.0], requires_grad=True) #I have change it from -2 to +2 otherwise there is a  division with denominator 0! \n",
    "x_4 = torch.tensor([-1.0], requires_grad=True)\n",
    "x_5 = torch.tensor([-2.0], requires_grad=True)\n",
    "\n",
    "print(x_1)\n",
    "print(x_2)\n",
    "print(x_3)\n",
    "print(x_4)\n",
    "print(x_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first evaluate the forward function step by step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7.], grad_fn=<AddBackward0>)\n",
      "tensor([1.], grad_fn=<AddBackward0>)\n",
      "tensor([0.6570], grad_fn=<SinBackward0>)\n",
      "tensor([1.], grad_fn=<ReluBackward0>)\n",
      "tensor([0.6570], grad_fn=<DivBackward0>)\n",
      "tensor([2.6570], grad_fn=<SubBackward0>)\n",
      "tensor([14.2533], grad_fn=<ExpBackward0>)\n"
     ]
    }
   ],
   "source": [
    "a = x_1 + x_2\n",
    "print(a)\n",
    "b = x_3 + x_4\n",
    "print(b)\n",
    "c = a.sin()\n",
    "print(c)\n",
    "d = torch.nn.functional.relu(b)\n",
    "print(d)\n",
    "e = c/d\n",
    "print(e)\n",
    "f = e - x_5\n",
    "print(f)\n",
    "g = f.exp()\n",
    "print(g)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the image of the manual calculations for the backward propagation:\n",
    "\n",
    "![](ex2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's verify the manual counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10.7456])\n",
      "tensor([10.7456])\n",
      "tensor([-9.3642])\n",
      "tensor([-9.3642])\n",
      "tensor([-14.2533])\n"
     ]
    }
   ],
   "source": [
    "g.backward()\n",
    "print(x_1.grad)\n",
    "print(x_2.grad)\n",
    "print(x_3.grad)\n",
    "print(x_4.grad)\n",
    "print(x_5.grad)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0f658439eb39566f305aee04d580d1e9b360828ab059e2dbe01d2bb3072c08a2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('DL_Lab')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
