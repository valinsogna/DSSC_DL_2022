{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework\n",
    "\n",
    "1. Complete the Python implementation of the backpropagation exercise in the **Backpropagation** section here above (cell `# try it in Python as homework!`)\n",
    "    - Create the calculations for obtaining $y$ in PyTorch **using only PyTorch methods and routines**\n",
    "    - Calculate the gradient\n",
    "    - Check the values of the gradients and see if it is correct w.r.t. the manual calculations\n",
    "2. Given the multilayer perceptron defined during the exercises from lab 1:\n",
    "    - Create 10 random datapoints (with any function you wish, it can be `rand`, `randn`...) and feed them into the network\n",
    "    - Given the output, calculate the Cross-Entropy loss with respect to the ground truth $[1,2,3,4,1,2,3,4,1,2]$ (classes from 1 to 4). Cross-Entropy loss:\n",
    "        \n",
    "        $$ CE(\\mathbf{y}, \\hat{\\mathbf{y}}) = - \\sum_{i=1}^{10} \\hat{y}_i \\log(y_i)$$\n",
    "        \n",
    "        where $y_i$ is the one-hot encoding of the $i$-th datapoint. For instance, $y_1 = [1,0,0,0]$.\n",
    "        **_Note: there is an extremely handy PyTorch function for getting a one-hot encoding out of a vector, so don't try anything fancy._**\n",
    "    - Backpropagate the error along the network and inspect the gradient of the parameters connecting the input layer and the first hidden layer.\n",
    "3. Execute the python script `utils/randomized_backpropagation_formula.py`. This creates a formula $f(\\mathbf{x})$ with randomized operators and values. Create the computational graph from this formula, do (by hand) the forward pass, then calculate (by hand) $\\nabla f(\\mathbf{x})$ using the backward gradient computation. Do the same calculation on PyTorch to check the correctness of your calculations. _Note: The formula created by this script is linked to your name and surname, which you have to input before_. The solution to this exercise _should_ be submitted as a scan/good quality picture of a piece of paper (or you can do it on a touch screen and submit the image...), but other formats are acceptable as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "**Backpropagation**\n",
    "\n",
    "Let us suppose we have the following calculation\n",
    "\n",
    "$\\mathbf{x} = [1,~2,~-1,~3,~5]$\n",
    "\n",
    "$ y = f(\\mathbf{x}) = \\log\\{[\\exp (x_1 * x_2 )]^2 + \\sin (x_3 + x_4 + x_5) \\cdot x_5\\}$\n",
    "\n",
    "Find\n",
    "\n",
    "$\\nabla f(\\mathbf{x})$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], requires_grad=True)\n",
      "tensor([2.], requires_grad=True)\n",
      "tensor([-1.], requires_grad=True)\n",
      "tensor([3.], requires_grad=True)\n",
      "tensor([5.], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "x1 = torch.tensor([1.0], requires_grad=True)\n",
    "x2 = torch.tensor([2.0], requires_grad=True)\n",
    "x3 = torch.tensor([-1.0], requires_grad=True)\n",
    "x4 = torch.tensor([3.0], requires_grad=True)\n",
    "x5 = torch.tensor([5.0], requires_grad=True)\n",
    "\n",
    "print(x1)\n",
    "print(x2)\n",
    "print(x3)\n",
    "print(x4)\n",
    "print(x5)\n",
    "\n",
    "# x = torch.tensor([1,2,-1,3,5], dtype=torch.float32, requires_grad=True)\n",
    "# print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2., grad_fn=<DotBackward0>)\n",
      "tensor([7.], grad_fn=<AddBackward0>)\n",
      "tensor(7.3891, grad_fn=<ExpBackward0>)\n",
      "tensor(54.5982, grad_fn=<PowBackward0>)\n",
      "tensor([0.6570], grad_fn=<SinBackward0>)\n",
      "tensor(3.2849, grad_fn=<DotBackward0>)\n",
      "tensor(57.8831, grad_fn=<AddBackward0>)\n",
      "\n",
      "\n",
      "The value of function f(x) evalkuated in x is:\n",
      "tensor(4.0584, grad_fn=<LogBackward0>)\n"
     ]
    }
   ],
   "source": [
    "a = torch.matmul(x1, x2)\n",
    "print(a)\n",
    "b = x3 + x4 + x5\n",
    "print(b)\n",
    "c = a.exp()\n",
    "print(c)\n",
    "d = torch.pow(c, 2)\n",
    "print(d)\n",
    "g = b.sin()\n",
    "print(g)\n",
    "h = torch.matmul(g, x5)\n",
    "print(h)\n",
    "i = d + h\n",
    "print(i)\n",
    "y = torch.log(i)\n",
    "print(\"\\n\")\n",
    "print(\"The value of function f(x) evaluated in x is:\")\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3.7730])\n",
      "tensor([1.8865])\n",
      "tensor([0.0651])\n",
      "tensor([0.0651])\n",
      "tensor([0.0765])\n"
     ]
    }
   ],
   "source": [
    "y.backward()\n",
    "print(x1.grad)\n",
    "print(x2.grad)\n",
    "print(x3.grad)\n",
    "print(x4.grad)\n",
    "print(x5.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From manual calculations performed in class, we expected:\n",
    "\n",
    "- $\\partial f/\\partial x_1 = 4.14$\n",
    "- $\\partial f/\\partial x_2 = 2.07$\n",
    "- $\\partial f/\\partial x_3 = 0.08$\n",
    "- $\\partial f/\\partial x_4 = 0.08$\n",
    "- $\\partial f/\\partial x_4 = 0.093$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from previous cell in Part 2, the manual calculations are slightly different since we made use of approximations in class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input your name, then press ENTER: Valeria Insogna\n",
    "\n",
    "f(X) =  exp((sin(x1 + x2) / ReLU(x3 + x4)) - x5)\n",
    "\n",
    "Your values\n",
    "{'x1': 3, 'x2': 4, 'x3': -2, 'x4': -1, 'x5': -2}\n",
    "\n",
    "Calculate âˆ‡f(X) [NB: if division by 0, change the value(s) of X responsible for that]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_1 = torch.tensor([3.0], requires_grad=True)\n",
    "x_2 = torch.tensor([4.0], requires_grad=True)\n",
    "x_3 = torch.tensor([2.0], requires_grad=True) #I have change it from -2 to +2 otherwise there is a  division with denominator 0! \n",
    "x_4 = torch.tensor([-1.0], requires_grad=True)\n",
    "x_5 = torch.tensor([-2.0], requires_grad=True)\n",
    "\n",
    "print(x_1)\n",
    "print(x_2)\n",
    "print(x_3)\n",
    "print(x_4)\n",
    "print(x_5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first evaluate the forward function step by step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = x_1 + x_2\n",
    "print(a)\n",
    "b = x_3 + x_4\n",
    "print(b)\n",
    "c = a.sin()\n",
    "print(c)\n",
    "d = torch.nn.functional.relu(b)\n",
    "print(d)\n",
    "e = c/d\n",
    "print(e)\n",
    "f = e - x_5\n",
    "print(f)\n",
    "g = f.exp()\n",
    "print(g)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the image of the manual calculations for the backward propagation:\n",
    "\n",
    "![](ex2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's verify the manual counts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g.backward()\n",
    "print(x_1.grad)\n",
    "print(x_2.grad)\n",
    "print(x_3.grad)\n",
    "print(x_4.grad)\n",
    "print(x_5.grad)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0f658439eb39566f305aee04d580d1e9b360828ab059e2dbe01d2bb3072c08a2"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('DL_Lab')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
