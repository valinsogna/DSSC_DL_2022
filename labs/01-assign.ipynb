{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework\n",
    "\n",
    "1. build the MLP in the image above using PT built-ins\n",
    "2. Provide calculation for the exact number of parameters of the MLP\n",
    "   - Do it first supposing that the layers don't have a bias term, then supposing that the bias is present wherever it's possible\n",
    "3. Calculate the $L_1$ (vectorial) norm and the Frobenius norm for the params of each layer\n",
    "4. Given 10 random datapoints, feed them into the network. This operation must be done all in one single command and must **not** make use of loops.\n",
    "   - Given the output of the network, using PyTorch code, find the class of assignment of each datapoint. This also must be done in a single PyTorch command without using loops.\n",
    "   - Drafting a vector of ground truths (whichever labels you like), provide code for the calculation of the accuracy\n",
    "     - Tip: first get the number of correct assignments, then..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recall\n",
    "Let us suppose we wish to build a larger model from the graph below.\n",
    "\n",
    "![](imgs/01/mlp_graph_larger.jpg)\n",
    "\n",
    "We suppose that\n",
    "\n",
    "1. The layers have no bias units\n",
    "2. The activation function for hidden layers is `ReLU`\n",
    "\n",
    "    $ \\text{ReLU}(x) = \\max(0, x)$\n",
    "\n",
    "Moreover, we suppose that this is a classification problem.\n",
    "\n",
    "As you might recall, when the number of classes is > 2, we encode the problem in such a way that the output layer has a no. of neurons corresponding to the no. of classes. Doing so, we establish a correspondence between output units and classes. The value of the $j$-th neuron represents the **confidence** of the network in assigning a given data instance to the $j$-th class.\n",
    "\n",
    "Classically, when the network is encoded in such way, the activation function for the final layer is the **softmax** function.\n",
    "If $C$ is the total number of classes,\n",
    "\n",
    "$softmax(z_j) = \\frac{\\exp(z_j)}{\\sum_{k=1}^C \\exp(z_k)}$\n",
    "\n",
    "where $j\\in \\{1,\\cdots,C\\}$ is one of the classes.\n",
    "\n",
    "If we repeat this calculation for all $j$ s, we end up with $C$ normalized values (i.e., between 0 and 1) which can be interpreted as a confidence that the network has in assigning the instance to the corresponding class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(torch.nn.Module):\n",
    "    def __init__(self, my_bias=False):\n",
    "        super().__init__()\n",
    "        self.layers = torch.nn.Sequential(\n",
    "            torch.nn.Linear(5, 11, bias=my_bias),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(11, 16, bias=my_bias),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(16, 13, bias=my_bias),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(13, 8, bias=my_bias),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(8, 4, bias=my_bias),\n",
    "            torch.nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, X):\n",
    "        return self.layers(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OSS.**\n",
    "CLASS torch.nn.Softmax(dim=None):\n",
    "\n",
    "Applies the Softmax function to an n-dimensional input Tensor rescaling them so that the elements of the n-dimensional output Tensor lie in the range [0,1] and sum to 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now test if MPL class works by instanciating one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=5, out_features=11, bias=False)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=11, out_features=16, bias=False)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=16, out_features=13, bias=False)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=13, out_features=8, bias=False)\n",
       "    (7): ReLU()\n",
       "    (8): Linear(in_features=8, out_features=4, bias=False)\n",
       "    (9): Softmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MLP()\n",
    "model # print"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a simple neural network model with no bias, if there are $i$ number of input variables, $o$ number of output variables and one hidden layer with $h$ neurons this conditions holds:\n",
    "\n",
    "\\begin{equation}\n",
    "\\# parameters = i * h + h * o\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, in our model with 4 hidden layers and with $i=5$, $o=4$, $h_1=11$, $h_2=16$, $h_3=13$ and $h_4=8$ we have:\n",
    "\n",
    "\\begin{equation}\n",
    "\\# parameters = i * h_1 + h_1 * h_2 + h_2 * h_3 * h_3 * h_4 * o = 575\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check it by means of torchinfo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchinfo import summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "MLP                                      --\n",
       "├─Sequential: 1-1                        --\n",
       "│    └─Linear: 2-1                       55\n",
       "│    └─ReLU: 2-2                         --\n",
       "│    └─Linear: 2-3                       176\n",
       "│    └─ReLU: 2-4                         --\n",
       "│    └─Linear: 2-5                       208\n",
       "│    └─ReLU: 2-6                         --\n",
       "│    └─Linear: 2-7                       104\n",
       "│    └─ReLU: 2-8                         --\n",
       "│    └─Linear: 2-9                       32\n",
       "│    └─Softmax: 2-10                     --\n",
       "=================================================================\n",
       "Total params: 575\n",
       "Trainable params: 575\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a simple neural network model **with bias** (in the output and hidden layers) and if there are $i$ number of input variables, $o$ number of output variables and one hidden layer with $h$ neurons this conditions holds:\n",
    "\n",
    "\\begin{equation}\n",
    "\\# parameters = i * h + h + h * o + o\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, in our model with 4 hidden layers and with $i=5$, $o=4$, $h_1=11$, $h_2=16$, $h_3=13$ and $h_4=8$ we have:\n",
    "\n",
    "\\begin{equation}\n",
    "\\# parameters = i * h_1 + h_1 * h_2 + h_2 * h_3 * h_3 * h_4 * o  + h_1 + h_2 + h_3 + h_4 + o = 627\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check it by means of torchinfo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (layers): Sequential(\n",
       "    (0): Linear(in_features=5, out_features=11, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=11, out_features=16, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=16, out_features=13, bias=True)\n",
       "    (5): ReLU()\n",
       "    (6): Linear(in_features=13, out_features=8, bias=True)\n",
       "    (7): ReLU()\n",
       "    (8): Linear(in_features=8, out_features=4, bias=True)\n",
       "    (9): Softmax(dim=1)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_with_bias = MLP(True)\n",
    "model_with_bias # print"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "=================================================================\n",
       "Layer (type:depth-idx)                   Param #\n",
       "=================================================================\n",
       "MLP                                      --\n",
       "├─Sequential: 1-1                        --\n",
       "│    └─Linear: 2-1                       66\n",
       "│    └─ReLU: 2-2                         --\n",
       "│    └─Linear: 2-3                       192\n",
       "│    └─ReLU: 2-4                         --\n",
       "│    └─Linear: 2-5                       221\n",
       "│    └─ReLU: 2-6                         --\n",
       "│    └─Linear: 2-7                       112\n",
       "│    └─ReLU: 2-8                         --\n",
       "│    └─Linear: 2-9                       36\n",
       "│    └─Softmax: 2-10                     --\n",
       "=================================================================\n",
       "Total params: 627\n",
       "Trainable params: 627\n",
       "Non-trainable params: 0\n",
       "================================================================="
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model_with_bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code produces the norm of each specific parameter $w_i$ in each layer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L1 norm of layers.0.weight :  tensor([1.3706, 1.3095, 1.2076, 1.3152, 1.0968, 0.7321, 0.8721, 1.1712, 1.3270,\n",
      "        0.8822, 1.0065])\n",
      "Frobenius norm of layers.0.weight :  tensor([0.6171, 0.6631, 0.5712, 0.6768, 0.5667, 0.3742, 0.4977, 0.5935, 0.7378,\n",
      "        0.4492, 0.5604])\n",
      "\n",
      "\n",
      "L1 norm of layers.2.weight :  tensor([1.8714, 1.6192, 1.8858, 1.5711, 2.1190, 1.8845, 1.6313, 1.3960, 1.5442,\n",
      "        2.1096, 2.0135, 1.3465, 1.6481, 1.3771, 1.4941, 1.8197])\n",
      "Frobenius norm of layers.2.weight :  tensor([0.6108, 0.5374, 0.6274, 0.5245, 0.6992, 0.6431, 0.5566, 0.5140, 0.5059,\n",
      "        0.6775, 0.6724, 0.4786, 0.5861, 0.5034, 0.5197, 0.6034])\n",
      "\n",
      "\n",
      "L1 norm of layers.4.weight :  tensor([2.1936, 1.7309, 2.1227, 2.3824, 2.1517, 2.3157, 1.6894, 2.4102, 1.9114,\n",
      "        2.2467, 1.5643, 2.1334, 2.1701])\n",
      "Frobenius norm of layers.4.weight :  tensor([0.6351, 0.5149, 0.6045, 0.6591, 0.5974, 0.6351, 0.5048, 0.6733, 0.5452,\n",
      "        0.6209, 0.4551, 0.6247, 0.6435])\n",
      "\n",
      "\n",
      "L1 norm of layers.6.weight :  tensor([2.1437, 1.4042, 1.8201, 1.8328, 1.8621, 2.1921, 1.9160, 1.7135])\n",
      "Frobenius norm of layers.6.weight :  tensor([0.6387, 0.4735, 0.5642, 0.6088, 0.6214, 0.6602, 0.6205, 0.5578])\n",
      "\n",
      "\n",
      "L1 norm of layers.8.weight :  tensor([1.6183, 1.1345, 1.0922, 0.9489])\n",
      "Frobenius norm of layers.8.weight :  tensor([0.6559, 0.4739, 0.5128, 0.4253])\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for param_name, param in model.state_dict().items(): \n",
    "    print(\"L1 norm of\", param_name, \": \", torch.norm(param, p=1, dim=1)) \n",
    "    print(\"Frobenius norm of\", param_name, \": \", torch.norm(param, p='fro', dim=1))\n",
    "    print(\"\\n\")\n",
    "    #print(param_name, param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feed 10 datapoints to the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5287,  0.1404,  1.0294,  0.6824, -1.9075],\n",
       "        [ 2.2157, -0.1230,  0.5900,  1.1393,  2.0579],\n",
       "        [ 1.7383,  2.6700,  2.2156,  2.4686,  1.4911],\n",
       "        [ 3.2361,  2.3077,  3.8812,  4.9305,  3.1369],\n",
       "        [ 3.9183,  3.5097,  1.9303,  4.8616,  2.9701],\n",
       "        [ 4.1286,  6.7141,  5.0085,  3.9030,  4.7227],\n",
       "        [ 5.8071,  5.6323,  5.8233,  6.4185,  5.4331],\n",
       "        [ 7.3633,  6.3232,  6.5968,  6.3240,  7.8792],\n",
       "        [ 7.6105,  8.0370,  8.7238,  6.5124, 10.0162],\n",
       "        [ 8.4191,  8.3492,  9.6653,  9.5996,  9.7586]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = torch.arange(0, 10).float().unsqueeze(-1)\n",
    "x2 = torch.arange(0, 10).float().unsqueeze(-1)\n",
    "x3 = torch.arange(0, 10).float().unsqueeze(-1)\n",
    "x4 = torch.arange(0, 10).float().unsqueeze(-1)\n",
    "x5 = torch.arange(0, 10).float().unsqueeze(-1)\n",
    "\n",
    "# Concatenate inputs:\n",
    "X = torch.cat((x1, x2, x3, x4, x5), dim=1)\n",
    "# Add noise\n",
    "eps = torch.normal(0, .8, (10, 5))\n",
    "X += (eps)\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2481, 0.2497, 0.2506, 0.2515],\n",
       "        [0.2423, 0.2516, 0.2499, 0.2563],\n",
       "        [0.2472, 0.2516, 0.2507, 0.2504],\n",
       "        [0.2416, 0.2503, 0.2576, 0.2505],\n",
       "        [0.2434, 0.2504, 0.2557, 0.2504],\n",
       "        [0.2448, 0.2554, 0.2474, 0.2524],\n",
       "        [0.2405, 0.2545, 0.2548, 0.2502],\n",
       "        [0.2376, 0.2571, 0.2531, 0.2522],\n",
       "        [0.2362, 0.2597, 0.2509, 0.2532],\n",
       "        [0.2335, 0.2573, 0.2586, 0.2506]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf_y_hat = model(X)\n",
    "conf_y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 3, 1, 2, 2, 1, 2, 1, 1, 2])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat = torch.argmax(conf_y_hat, dim=1)\n",
    "y_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get ground truths vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 3, 0, 1, 1, 0, 0, 2, 2])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.randint(4, (10, ))\n",
    "#y = torch.Tensor([2,2,2,2,2,2,2,2,2,2])\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate di accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y, y_hat):\n",
    "    correctly_classified = torch.sum(y_hat == y).item()\n",
    "    #print(correctly_classified)\n",
    "    #correctly_classified = (y_hat == y).sum()\n",
    "    return (correctly_classified / y.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy(y, y_hat)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "48db1f4229b39565c08a63548e3676c0a20093870abafd4b6886e5705c3c835d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('DL_lab')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
